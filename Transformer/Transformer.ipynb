{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce527d2-2355-441f-8e05-766749d794cf",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b4ba26-a891-445c-9688-3e344c3f546d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"Transformer\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5085faa2-607e-4376-bab7-0bacb3ffaeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1ddbb9c-b958-4d1f-9623-b68dd5ee8024",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim) # Embedding layer\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Initialize weights to improve training\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize embedding weights using normal distribution with small standard deviation\"\"\"\n",
    "        nn.init.normal_(self.embed.weight, mean=0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices [batch_size, seq_len]\n",
    "        Returns:\n",
    "            embeddings: Token embeddings [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        # Scale embeddings by sqrt(embed_dim) to stabilize gradients\n",
    "        return self.embed(x) * math.sqrt(self.embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53002812-0c91-4dae-ab02-308a85bbc406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        # Create a matrix of shape (max_seq_len, embed_dim)\n",
    "        pe = torch.zeros(max_seq_len, embed_dim)\n",
    "        # Create a vector of shape (max_seq_len, 1)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)  #(max_seq_len, 1)\n",
    "        # Calculate the division term: 10000^(2i/embed_dim)\n",
    "        div_term = torch.pow(10000, torch.arange(0, embed_dim, 2).float() / embed_dim)  #(embed_dim/2)\n",
    "\n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position / div_term) #(max_seq_len, embed_dim/2)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term) #(max_seq_len, embed_dim/2)\n",
    "\n",
    "        # Add a batch dimension\n",
    "        pe = pe.unsqueeze(0) #(1, max_seq_len, embed_dim)\n",
    "\n",
    "        # Register buffer makes the parameter persistent but not a model parameter\n",
    "        # that is updated during training\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input embeddings\n",
    "        x = x + self.pe[:, :x.shape[1], :] # `pe` is not trainable but moves with the model\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b310a34-6e45-497c-bc85-4b1600752d12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Final output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None, return_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Query tensor [batch_size, query_len, embed_dim]\n",
    "            key: Key tensor [batch_size, key_len, embed_dim]\n",
    "            value: Value tensor [batch_size, value_len, embed_dim]\n",
    "            mask: Optional mask tensor for masked attention\n",
    "            return_attention: Whether to return attention weights\n",
    "        Returns:\n",
    "            output: Attention output [batch_size, query_len, embed_dim]\n",
    "            attention_weights: (Optional) Attention weights\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0) \n",
    "        query_len = query.size(1)\n",
    "        key_len = key.size(1)\n",
    "        value_len = value.size(1)\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        q = self.q_linear(query)  # [batch_size, query_len, embed_dim]\n",
    "        k = self.k_linear(key)    # [batch_size, key_len, embed_dim]\n",
    "        v = self.v_linear(value)  # [batch_size, value_len, embed_dim]\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # Split embed_dim into num_heads Ã— head_dim\n",
    "        q = q.view(batch_size, query_len, self.num_heads, self.head_dim).transpose(1, 2) # q shape: [batch_size, num_heads, query_len, head_dim]\n",
    "        k = k.view(batch_size, key_len, self.num_heads, self.head_dim).transpose(1, 2) # k shape: [batch_size, num_heads, key_len, head_dim]\n",
    "        v = v.view(batch_size, value_len, self.num_heads, self.head_dim).transpose(1, 2) # v shape: [batch_size, num_heads, value_len, head_dim]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1))  # [batch_size, num_heads, query_len, key_len]\n",
    "        \n",
    "        # Scale attention scores\n",
    "        attn_scores = attn_scores / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        # mask shape: [batch_size, seq_len] or [batch_size, 1, seq_len] or [batch_size, seq_len, seq_len]\n",
    "        if mask is not None:\n",
    "            # Expand mask to match the attention scores dimensions\n",
    "            if mask.dim() == 2:\n",
    "                # [batch_size, seq_len] -> [batch_size, 1, 1, seq_len]\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            elif mask.dim() == 3 and mask.size(1) == 1:\n",
    "                # [batch_size, 1, seq_len] -> [batch_size, 1, 1, seq_len]\n",
    "                mask = mask.unsqueeze(1)\n",
    "            elif mask.dim() == 3:\n",
    "                # [batch_size, seq_len, seq_len] -> [batch_size, 1, seq_len, seq_len]\n",
    "                mask = mask.unsqueeze(1)\n",
    "            \n",
    "            # Apply mask by setting masked positions to -inf before softmax\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf')) # [batch_size, num_heads, seq_len, seq_len]\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attn_scores, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        attn_output = torch.matmul(attention_weights, v)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # Reshape back to original dimension\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, query_len, self.embed_dim)\n",
    "        # attn_output shape: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.out_proj(attn_output)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attention_weights\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04bb1364-ee34-438c-8f56-f46cfde9e5bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_hidden_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Position-wise feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Input and output dimension\n",
    "            ff_hidden_dim: Hidden layer dimension\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_hidden_dim)\n",
    "        self.linear2 = nn.Linear(ff_hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, embed_dim]\n",
    "            \n",
    "        Returns:\n",
    "            output: Output tensor [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "410a3091-a616-40a8-a918-9eb1a8722eb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Single transformer encoder layer with self-attention and feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Dimension of embeddings\n",
    "            num_heads: Number of attention heads\n",
    "            ff_hidden_dim: Hidden dimension of feed-forward network\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = FeedForward(embed_dim, ff_hidden_dim, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, embed_dim]\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            x: Output tensor [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        # Self-attention block with residual connection and layer normalization\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = residual + self.dropout(self.self_attn(x, x, x, mask))\n",
    "        \n",
    "        # Feed-forward block with residual connection and layer normalization\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = residual + self.dropout(self.ff(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53fe1545-eb29-4d37-95e1-3d572230fd66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Single transformer decoder layer with masked self-attention, \n",
    "        cross-attention, and feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Dimension of embeddings\n",
    "            num_heads: Number of attention heads\n",
    "            ff_hidden_dim: Hidden dimension of feed-forward network\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.ff = FeedForward(embed_dim, ff_hidden_dim, dropout)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of decoder layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, embed_dim]\n",
    "            enc_output: Encoder output [batch_size, enc_seq_len, embed_dim]\n",
    "            self_attn_mask: Mask for self-attention (usually causal mask)\n",
    "            cross_attn_mask: Mask for cross-attention\n",
    "            \n",
    "        Returns:\n",
    "            x: Output tensor [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        # Self-attention block with residual connection and layer normalization\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = residual + self.dropout(self.self_attn(x, x, x, self_attn_mask))\n",
    "        \n",
    "        # Cross-attention block with residual connection and layer normalization\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = residual + self.dropout(self.cross_attn(x, enc_output, enc_output, cross_attn_mask))\n",
    "        \n",
    "        # Feed-forward block with residual connection and layer normalization\n",
    "        residual = x\n",
    "        x = self.norm3(x)\n",
    "        x = residual + self.dropout(self.ff(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2298b838-d222-4c89-b104-f5e1ff8b1171",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, max_seq_len, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Full transformer encoder consisting of embedding layer, positional encoding, \n",
    "        and stack of encoder layers.\n",
    "        \n",
    "        Args:\n",
    "            max_seq_len: Maximum sequence length\n",
    "            vocab_size: Size of vocabulary\n",
    "            embed_dim: Dimension of embeddings\n",
    "            num_heads: Number of attention heads\n",
    "            ff_hidden_dim: Hidden dimension of feed-forward network\n",
    "            num_layers: Number of encoder layers\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_seq_len)\n",
    "        \n",
    "        # Create stack of encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Input token indices [batch_size, seq_len]\n",
    "            mask: Optional padding mask\n",
    "            \n",
    "        Returns:\n",
    "            x: Encoder output [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        # Convert tokens to embeddings and add positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through each encoder layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            \n",
    "        # Final layer normalization\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eab8c1bd-9f6e-4b4f-8da0-1cddfe11bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, max_seq_len, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Full transformer decoder consisting of embedding layer, positional encoding,\n",
    "        and stack of decoder layers.\n",
    "        \n",
    "        Args:\n",
    "            max_seq_len: Maximum sequence length\n",
    "            vocab_size: Size of vocabulary\n",
    "            embed_dim: Dimension of embeddings\n",
    "            num_heads: Number of attention heads\n",
    "            ff_hidden_dim: Hidden dimension of feed-forward network\n",
    "            num_layers: Number of decoder layers\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_seq_len)\n",
    "        \n",
    "        # Create stack of decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.output_proj = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, enc_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Input token indices [batch_size, seq_len]\n",
    "            enc_output: Encoder output [batch_size, enc_seq_len, embed_dim]\n",
    "            self_attn_mask: Mask for self-attention (usually causal mask)\n",
    "            cross_attn_mask: Mask for cross-attention\n",
    "            \n",
    "        Returns:\n",
    "            output: Output logits [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        # Convert tokens to embeddings and add positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through each decoder layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, self_attn_mask, cross_attn_mask)\n",
    "            \n",
    "        # Final layer normalization and projection to vocabulary size\n",
    "        x = self.norm(x)\n",
    "        return self.output_proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "954aa7b9-86ef-4257-bd0c-d98de2768c79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, max_seq_len=512, \n",
    "                 embed_dim=512, num_heads=8, ff_hidden_dim=2048, \n",
    "                 num_encoder_layers=6, num_decoder_layers=6, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Complete Transformer model with encoder and decoder.\n",
    "        \n",
    "        Args:\n",
    "            src_vocab_size: Size of source vocabulary\n",
    "            tgt_vocab_size: Size of target vocabulary\n",
    "            max_seq_len: Maximum sequence length\n",
    "            embed_dim: Dimension of embeddings\n",
    "            num_heads: Number of attention heads\n",
    "            ff_hidden_dim: Hidden dimension of feed-forward network\n",
    "            num_encoder_layers: Number of encoder layers\n",
    "            num_decoder_layers: Number of decoder layers\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            max_seq_len=max_seq_len,\n",
    "            vocab_size=src_vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            ff_hidden_dim=ff_hidden_dim,\n",
    "            num_layers=num_encoder_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.decoder = TransformerDecoder(\n",
    "            max_seq_len=max_seq_len,\n",
    "            vocab_size=tgt_vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            ff_hidden_dim=ff_hidden_dim,\n",
    "            num_layers=num_decoder_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "    def create_masks(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Create necessary masks for transformer training.\n",
    "        \n",
    "        Args:\n",
    "            src: Source sequence [batch_size, src_len]\n",
    "            tgt: Target sequence [batch_size, tgt_len]\n",
    "            \n",
    "        Returns:\n",
    "            src_mask: Source padding mask\n",
    "            tgt_mask: Target padding and causal mask\n",
    "            src_tgt_mask: Source-target padding mask for cross-attention\n",
    "        \"\"\"\n",
    "        # Create padding masks\n",
    "        src_pad_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, src_len)\n",
    "        tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, tgt_len)\n",
    "        \n",
    "        # Create causal mask for decoder's self-attention\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_causal_mask = torch.tril(torch.ones(tgt_len, tgt_len)).to(tgt.device)\n",
    "        tgt_causal_mask = tgt_causal_mask.unsqueeze(0).unsqueeze(1)  # (1, 1, tgt_len, tgt_len)\n",
    "        \n",
    "        # Combine padding mask and causal mask for decoder's self-attention\n",
    "        tgt_mask = tgt_pad_mask & tgt_causal_mask\n",
    "        \n",
    "        # Create mask for decoder's cross-attention\n",
    "        src_tgt_mask = src_pad_mask.transpose(-2, -1)  # (batch_size, 1, src_len, 1)\n",
    "        \n",
    "        return src_pad_mask, tgt_mask, src_tgt_mask\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Forward pass of the full transformer model.\n",
    "        \n",
    "        Args:\n",
    "            src: Source sequence [batch_size, src_len]\n",
    "            tgt: Target sequence [batch_size, tgt_len]\n",
    "            \n",
    "        Returns:\n",
    "            output: Output logits [batch_size, tgt_len, tgt_vocab_size]\n",
    "        \"\"\"\n",
    "        # Create masks for attention\n",
    "        src_mask, tgt_mask, src_tgt_mask = self.create_masks(src, tgt)\n",
    "        \n",
    "        # Encoder forward pass\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        \n",
    "        # Decoder forward pass\n",
    "        output = self.decoder(tgt, enc_output, tgt_mask, src_tgt_mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc62026-f884-45b0-91a0-ebe53c906858",
   "metadata": {},
   "source": [
    "### Types of Masks:\n",
    "1. **Source Padding Mask `(src_pad_mask)`:** This mask is used to mask out the padding tokens in the source sequence (those with value `0`), ensuring that padding tokens do not influence the attention calculations.\n",
    "2. **Target Padding Mask `(tgt_pad_mask)`:** Similar to the source padding mask, this mask ensures that padding tokens in the target sequence (with value `0`) are not attended to during the self-attention operation in the decoder.\n",
    "3. **Target Causal Mask `(tgt_causal_mask)`:** This is a triangular mask used for the self-attention layer in the decoder to ensure that the prediction at time `t` does not depend on future tokens (i.e., the model cannot look ahead).\n",
    "4. **Source-Target Mask `(src_tgt_mask)`:** This mask is used in the cross-attention layer in the decoder to ensure that the source sequence padding is not attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f461d04-aa93-44b9-bc2b-11a626bdbde1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def transformer_example():\n",
    "    # Define model parameters\n",
    "    src_vocab_size = 10000\n",
    "    tgt_vocab_size = 10000\n",
    "    max_seq_len = 512\n",
    "    embed_dim = 512\n",
    "    num_heads = 8\n",
    "    ff_hidden_dim = 2048\n",
    "    num_encoder_layers = 6\n",
    "    num_decoder_layers = 6\n",
    "    \n",
    "    # Create model\n",
    "    model = Transformer(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        ff_hidden_dim=ff_hidden_dim,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers\n",
    "    )\n",
    "    \n",
    "    # Example input data\n",
    "    batch_size = 16\n",
    "    src_len = 32\n",
    "    tgt_len = 24\n",
    "    \n",
    "    src = torch.randint(1, src_vocab_size, (batch_size, src_len))\n",
    "    tgt = torch.randint(1, tgt_vocab_size, (batch_size, tgt_len))\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(src, tgt)\n",
    "    \n",
    "    print(f\"Source shape: {src.shape}\")\n",
    "    print(f\"Target shape: {tgt.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb4331-3ddc-4df9-96c5-d64cc9fec2fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b9d54-0d9a-4a47-9564-6f764b6d2370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
