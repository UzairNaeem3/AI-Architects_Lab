{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa226d2-4b44-4da6-bdaf-7102f7542e15",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://www.researchgate.net/profile/Ehsan-Amjadian/publication/352239001/figure/fig1/AS:1033334390013952@1623377525434/Detailed-view-of-a-transformer-encoder-block-It-first-passes-the-input-through-an.jpg\" alt=\"Transformer Encoder\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95754810-59bd-4703-be06-db559be46b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e80dc-e328-4774-9d2f-95f73e2c33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing classes from the respective notebooks\n",
    "%run 1_Embeddings.ipynb\n",
    "%run 2_Positional_Encoding.ipynb\n",
    "%run 4_Multihead_Attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e3f0ac2-bece-4f1b-95da-249c4a3c1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_x = x  # Residual connection before attention\n",
    "        attn_out = self.attn(x)\n",
    "        x = self.norm1(residual_x + self.dropout(attn_out))\n",
    "        \n",
    "        residual_x = x  # Residual connection before feedforward\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(residual_x + self.dropout(ff_out))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce2c7100-f8a6-4cc2-a5bd-67f3f8a6e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_len : length of input sequence\n",
    "        vocab_size: size of vocabulary\n",
    "        embed_dim: dimension of embedding\n",
    "        num_layers: number of encoder layers\n",
    "        ff_hidden_dim: feed forward hidden layer dimension\n",
    "        n_heads: number of heads in multihead attention\n",
    "        \n",
    "    Returns:\n",
    "        out: output of the encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_seq_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "406e134b-3d80-463c-91d8-26aace47a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 200\n",
    "vocab_size = 1000\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "batch_size = 32\n",
    "ffn_hidden = 2048\n",
    "num_layers = 6\n",
    "drop_prob = 0.2\n",
    "\n",
    "encoder = TransformerEncoder(max_seq_len, vocab_size, embed_dim, num_heads, ffn_hidden, num_layers, drop_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a09f1b-bda2-479e-b254-d1704f0c47af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (embedding): Embedding(\n",
       "    (embed): Embedding(1000, 512)\n",
       "  )\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fde7704-a203-4867-8dad-a56bef273d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 19427328\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "print(f\"Total Trainable Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c141feb-54ee-4ba3-9001-5a54d7435d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
