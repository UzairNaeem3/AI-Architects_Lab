{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcaae00-1e98-4838-bfdb-6203741afbba",
   "metadata": {},
   "source": [
    "## Self Attention in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd150c7-56e7-46cd-8b1c-9ced2e10e4b5",
   "metadata": {},
   "source": [
    "Self-attention helps a model **pay attention** to different parts of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe2f43-2e84-4455-9b5e-2f2ce5c78fad",
   "metadata": {},
   "source": [
    "### Imagine a Sentence:\n",
    "\n",
    "Letâ€™s say we have a sentence: **The cat sat on the mat.**\n",
    "\n",
    "Self-attention helps the model figure out which words in this sentence should be focused on when processing each word. For example, when processing the word **cat**, the model might pay attention to words like **the** (before it) and **sat** (after it) to understand its meaning better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eea498-b141-4ea3-b967-5153c4aa1787",
   "metadata": {},
   "source": [
    "### The Process:\n",
    "\n",
    "1. **Input:** The sentence is broken down into words: \"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\".\n",
    "2. **Embedding:** Each word gets converted into a vector.\n",
    "3. **Queries, Keys, and Values:**\n",
    "   - Each word gets three vectors: Query (Q), Key (K), and Value (V). These are created by multiplying the original word vectors by different weights.\n",
    "   - The **Query (Q)** vector is the word you're currently focusing on.\n",
    "   - The **Key (K)** vector tells how similar another word is to your current word.\n",
    "   - The **Value (V)** vector contains the meaning or information of the word you're looking at.\n",
    "4. **Attention Mechanism:**\n",
    "   - For each word, it compares its **Query (Q)** vector with the **Key (K)** vectors of all the other words to figure out how much attention each word should get.\n",
    "   - This comparison usually involves calculating a **score** that tells how much attention each word should give to others.\n",
    "   - These scores are then used to calculate a weighted average of the **Value (V)** vectors, which gives the output for that word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b27a3-eeaf-441b-96ae-ff7bee175b78",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4732d-fc18-4d98-8f33-9b28a8f2af53",
   "metadata": {},
   "source": [
    "#### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fc8334-836d-4144-8633-6cdf08a1372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a053a21-ebb7-422b-9db2-169df6e80483",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, d_k, d_v = 4, 8, 8\n",
    "Q = np.random.randn(seq_len, d_k) # Q (Query): shape (seq_len, d_k) -> (4, 8)\n",
    "K = np.random.randn(seq_len, d_k) # K (Key): shape (seq_len, d_k) -> (4, 8)\n",
    "V = np.random.randn(seq_len, d_v) # V (Value): shape (seq_len, d_v) -> (4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd5e9ce-4f54-4f36-a212-6f344541c344",
   "metadata": {},
   "source": [
    "Here, **seq_len = 4** indicates that the sentence or sequence consists of 4 tokens (words) and **d_k = 8** is the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4efe2d-cf88-40d7-9269-e030c915d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-1.55805137e+00  1.74599065e+00  6.13182772e-03  9.16655390e-02\n",
      "  -4.19406664e-01  1.20209171e+00 -3.22952013e-01 -1.57484697e+00]\n",
      " [-6.03090864e-01 -1.39311745e+00  5.41038476e-04  8.68501500e-01\n",
      "   1.01740729e+00 -1.23332745e+00  1.31092668e+00  1.07903594e+00]\n",
      " [ 4.15527794e-01  2.58613492e-01  1.41064692e+00  1.89567492e+00\n",
      "  -1.88668455e-01  1.22294262e+00  1.73747406e+00  1.29566400e+00]\n",
      " [-1.65589298e+00  9.20265360e-01  1.67641560e-01  1.55976837e+00\n",
      "   6.85358948e-02  1.00433167e+00  3.21650406e-01 -1.95328813e-01]]\n",
      "K\n",
      " [[ 0.31691824 -0.74019652 -0.16705587  1.15992309 -1.55371717 -0.86020114\n",
      "   0.75528109 -0.38178461]\n",
      " [ 0.2937555  -1.16828174  0.65689693  0.63906619  0.52966841  0.56031207\n",
      "  -0.1390223  -0.35731796]\n",
      " [-0.14005376  1.02710414  0.37303933 -0.28437786 -1.08703429  0.01543534\n",
      "  -0.38521217  0.77901054]\n",
      " [ 1.85600985  1.40490388 -0.41918586  0.43513747  1.02341881  0.765293\n",
      "   1.34713076  0.31635243]]\n",
      "V\n",
      " [[ 0.99755393  1.56166281  0.2986221   0.47543551  0.81800117  0.05950314\n",
      "  -0.11690738 -0.1013089 ]\n",
      " [-2.39818972  0.24701376 -1.32360787 -0.35113535 -0.05594357  0.58126118\n",
      "  -1.77497531 -0.77299625]\n",
      " [ 1.09046411 -0.02865198 -1.0261337  -0.31014785 -1.80174376 -1.98862992\n",
      "   0.40271034  1.33953195]\n",
      " [-0.2376382  -1.61614974 -1.45608631  0.05206926  0.69175483  0.99239482\n",
      "   1.44958549  0.17417298]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\", Q)\n",
    "print(\"K\\n\", K)\n",
    "print(\"V\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dba55a-c391-4b21-9249-5f6deb038e27",
   "metadata": {},
   "source": [
    "#### Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20346724-4c88-497a-b77f-37cd5ca19fb2",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf30d97-4c17-476e-8103-4556d4a335f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.70591902, -1.37586814,  1.35979172, -0.84403445],\n",
       "       [ 1.9056601 ,  1.28581105, -2.38259031, -0.49412577],\n",
       "       [ 1.9622213 ,  1.83882766,  0.75857093,  4.86142051],\n",
       "       [-0.07765669,  0.16947995,  0.46103018,  0.03823273]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(Q, K.T) # Q has shape (4, 8) and K.T has shape (8, 4), so the result will have shape (4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c896c3-133a-404b-aa61-8866dfeedfc9",
   "metadata": {},
   "source": [
    "**Why we need sqrt(d_k) in denominator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ec5bcd-61e4-4335-a4db-1797e50d12b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q variance: 1.0633619664558474\n",
      "K variance: 0.6293201691477228\n",
      "Q.K(T) variance: 2.9059453138911335\n"
     ]
    }
   ],
   "source": [
    "print(\"Q variance:\", Q.var()) \n",
    "print(\"K variance:\", K.var())\n",
    "print(\"Q.K(T) variance:\", np.matmul(Q, K.T).var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c0af85-2bb0-4147-9d05-47a613945c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q variance: 1.0633619664558474\n",
      "K variance: 0.6293201691477228\n",
      "normalized Q.K(T) variance: 0.3632431642363916\n"
     ]
    }
   ],
   "source": [
    "normalized = np.matmul(Q, K.T) / math.sqrt(d_k)\n",
    "print(\"Q variance:\", Q.var()) \n",
    "print(\"K variance:\", K.var())\n",
    "print(\"normalized Q.K(T) variance:\", normalized.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbeb713-cfaf-4325-9a26-ba5721ffd116",
   "metadata": {},
   "source": [
    "**Notice the reduction in variance of the product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f27ec487-a314-4b92-b62b-5d09db52fbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60313345, -0.48644285,  0.48075897, -0.29841124],\n",
       "       [ 0.67375259,  0.45460286, -0.84237288, -0.17469984],\n",
       "       [ 0.69374999,  0.65012375,  0.26819532,  1.71877171],\n",
       "       [-0.02745579,  0.05992021,  0.16299878,  0.01351731]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abb7d2-9665-4dbb-a82a-6e7aa6e81dd7",
   "metadata": {},
   "source": [
    "#### Masking\n",
    "- This is to ensure words don't get context from words generated in the future.\n",
    "- Not required in the encoders, but required int the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a97700e-fa1d-4988-bebc-f7513f9a098d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1.],\n",
       "       [0., 0., 1., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c285a58-f9a1-4ee5-88c4-c4f30c53aa17",
   "metadata": {},
   "source": [
    "The masking dimension is usually the same as the sequence length dimension **seq_len**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7661c28a-7171-4cb6-aea1-b72a8b4316a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 1] = -np.inf\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4257f1d-a956-4359-978a-77cd8bd7acf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60313345,        -inf,        -inf,        -inf],\n",
       "       [ 0.67375259,  0.45460286,        -inf,        -inf],\n",
       "       [ 0.69374999,  0.65012375,  0.26819532,        -inf],\n",
       "       [-0.02745579,  0.05992021,  0.16299878,  0.01351731]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c7ebd-58cd-4751-91fd-7ef0145b4f42",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dee225fa-a054-4d97-8f42-320c9c727281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    sum = exp_x.sum()\n",
    "    softmax = exp_x/sum\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42512d3-1d89-4c4e-9a27-f683c01826a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(normalized + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ded83a33-9546-406a-b414-603d75b11183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04042334, 0.        , 0.        , 0.        ],\n",
       "       [0.14493617, 0.11641294, 0.        , 0.        ],\n",
       "       [0.14786369, 0.14155164, 0.09661521, 0.        ],\n",
       "       [0.07188625, 0.07844997, 0.08696795, 0.07489283]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "153379f0-693c-4876-9f25-d855de03b88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04032446,  0.06312763,  0.0120713 ,  0.01921869,  0.03306634,\n",
       "         0.00240532, -0.00472579, -0.00409524],\n",
       "       [-0.13459867,  0.25509703, -0.11080394,  0.0280311 ,  0.1120454 ,\n",
       "         0.07629048, -0.2235742 , -0.10467009],\n",
       "       [-0.08661027,  0.26311021, -0.24234362, -0.00936914, -0.06104208,\n",
       "        -0.10105507, -0.22962908,  0.00502036],\n",
       "       [-0.03938946,  0.00811048, -0.28066134, -0.01644259, -0.05047221,\n",
       "        -0.04874643, -0.00406434,  0.0616164 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_V = np.matmul(attention, V)\n",
    "new_V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445975d4-23bb-487b-914f-64152051a022",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873c35f-f375-40be-ba4a-62e01f312a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def self_attention(Q, K, V, mask=None):\n",
    "  d_k = Q.shape[-1]\n",
    "  normalized = np.matmul(Q, K.T) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    normalized = normalized + mask\n",
    "  attention = softmax(normalized)\n",
    "  out = np.matmul(attention, V)\n",
    "  return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af3580-f14f-4c9e-b77a-e1a8ddd29c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, attention = self_attention(Q, K, V, mask=mask)\n",
    "print(\"Q\\n\", Q)\n",
    "print(\"K\\n\", K)\n",
    "print(\"V\\n\", V)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ddfb4-92b9-43db-ae07-f5dcde447adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
