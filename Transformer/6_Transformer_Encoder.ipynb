{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "785bcf2d-b6a8-4cdb-8a3b-1344b7f0f2ec",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa226d2-4b44-4da6-bdaf-7102f7542e15",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://media.datacamp.com/legacy/v1704797298/image_3aa5aef3db.png\" alt=\"Transformer Encoder\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95754810-59bd-4703-be06-db559be46b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e80dc-e328-4774-9d2f-95f73e2c33aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing classes from the respective notebooks\n",
    "%run 4_Multihead_Attention.ipynb\n",
    "%run 5_FeedForward.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3f0ac2-bece-4f1b-95da-249c4a3c1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Single transformer encoder layer with self-attention and feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Dimension of embeddings\n",
    "            num_heads: Number of attention heads\n",
    "            ff_hidden_dim: Hidden dimension of feed-forward network\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = FeedForward(embed_dim, ff_hidden_dim, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, embed_dim]\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            x: Output tensor [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        # Self-attention block with residual connection and layer normalization\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = residual + self.dropout(self.self_attn(x, x, x, mask))\n",
    "        \n",
    "        # Feed-forward block with residual connection and layer normalization\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = residual + self.dropout(self.ff(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c141feb-54ee-4ba3-9001-5a54d7435d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "# random input tensor with shape [batch_size, seq_len, embed_dim]\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "encoder_layer = EncoderLayer(embed_dim=embed_dim, num_heads=num_heads, ff_hidden_dim=ff_hidden_dim, dropout=dropout)\n",
    "output = encoder_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d2bb2-a455-4f8c-8053-f33f0ade4ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
