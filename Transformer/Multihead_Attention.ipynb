{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38fca610-8d73-4d6a-8d10-67b19acdf411",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0fbb9f-cb30-473b-b3bd-c2a760450411",
   "metadata": {},
   "source": [
    "Multi-Head Attention model works by applying multiple attention mechanisms (or \"heads\") in parallel to the input sequence. Each head learns to focus on different parts of the input independently, and their outputs are concatenated and passed through a linear layer to produce the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ecc86-c6b8-48a5-8b4c-7e9bbf21e721",
   "metadata": {},
   "source": [
    "#### Example Configuration\n",
    "- **batch_size = 32**\n",
    "- **num_heads = 8**\n",
    "- **d_model = 512**\n",
    "- **seq_len = 10**\n",
    "\n",
    "Let's see the dimensions step-by-step based on these values.\n",
    "\n",
    "#### 1. Input\n",
    "The input to the attention layer is a matrix of shape **(batch_size, seq_len, d_model)**, where:\n",
    "\n",
    "- **batch_size**: Number of samples in the batch.\n",
    "- **seq_len**: Length of the input sequence.\n",
    "- **d_model**: Dimensionality of the input embeddings.\n",
    "\n",
    "For example:\n",
    "- Input shape: **(32, 10, 512)** (batch size = 32, sequence length = 10, and model dimension = 512).\n",
    "\n",
    "#### 2. Linear Projections for Q, K, and V\n",
    "The input is projected into three matrices: **Query (Q)**, **Key (K)**, and **Value (V)**, each of dimension **(batch_size, seq_len, d_model)**.\n",
    "\n",
    "Each of Q, K, and V are obtained by multiplying the input matrix by learnable weight matrices, so the dimensions of each are still **(batch_size, seq_len, d_model)**.\n",
    "\n",
    "For example:\n",
    "- Q, K, V shapes: **(32, 10, 512)**.\n",
    "\n",
    "#### 3. Splitting into Heads\n",
    "To apply multi-head attention, we split the model dimension **d_model** into **num_heads** heads. Suppose we want **num_heads = 8** and **d_model = 512**, so each head will have a dimension of **d_head = d_model / num_heads = 512 / 8 = 64**.\n",
    "\n",
    "The matrices Q, K, and V are split into **num_heads** parts, resulting in each head having a dimension of **(batch_size, seq_len, d_head)**.\n",
    "\n",
    "For example:\n",
    "- Q, K, V shapes after splitting: **(32, 10, 64)** for each head.\n",
    "\n",
    "Now, we have **num_heads = 8** heads, so each head has a shape of **(32, 10, 64)**. For 8 heads, this becomes a shape of **(batch_size, num_heads, seq_len, d_head)**.\n",
    "\n",
    "For example:\n",
    "- Q, K, V for 8 heads: **(32, 8, 10, 64)**.\n",
    "\n",
    "#### 4. Scaled Dot-Product Attention\n",
    "Each head computes the scaled dot-product attention between the corresponding Query (Q) and Key (K) matrices. The result of this attention is a matrix of shape **(batch_size, num_heads, seq_len, seq_len)**.\n",
    "\n",
    "- The **Q** matrix has the shape **(batch_size, num_heads, seq_len, d_head)**, and the **K** matrix has the shape **(batch_size, num_heads, seq_len, d_head)**.\n",
    "- When performing the attention, we compute the dot product between each **Query** and **Key** for every position in the sequence. This results in a matrix of size **(batch_size, num_heads, seq_len, seq_len)**, where each **seq_len** in the rows corresponds to the attention of a query position, and the **seq_len** in the columns corresponds to the attention to all key positions.\n",
    "\n",
    "Thus, the output shape is **(batch_size, num_heads, seq_len, seq_len)**.\n",
    "\n",
    "For example:\n",
    "- Attention output for each head: **(32, 8, 10, 10)**.\n",
    "\n",
    "#### 5. Concatenation of Heads\n",
    "After performing attention on each head, the results of all heads are concatenated. The resulting shape is **(batch_size, seq_len, d_model)**.\n",
    "\n",
    "For example, for 8 heads with **d_head = 64** each:\n",
    "- Concatenated output shape: **(32, 10, 512)**.\n",
    "\n",
    "#### 6. Final Linear Projection\n",
    "The concatenated output is passed through a final linear layer to project it back into a **d_model**-dimensional space, which gives the final output shape as **(batch_size, seq_len, d_model)**.\n",
    "\n",
    "For example:\n",
    "- Final output shape: **(32, 10, 512)**.\n",
    "\n",
    "#### Recap of Dimensions:\n",
    "- **Input**: **(32, 10, 512)**\n",
    "- **Q, K, V**: **(32, 10, 512)** (before splitting)\n",
    "- **Q, K, V per head**: **(32, 8, 10, 64)** (after splitting into 8 heads)\n",
    "- **Attention output**: **(32, 8, 10, 10)**\n",
    "- **Concatenated output**: **(32, 10, 512)**\n",
    "- **Final output**: **(32, 10, 512)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39440ee7-d11e-499a-a142-ce29ce968650",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9de830c-3805-481a-8349-a780c77cb9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d87c1a4-60ed-407d-8377-38901e7728db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Configuration\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_head = d_model // num_heads  # d_head = 512 / 8 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fcfdd50-f8d5-47a2-a731-d98eacc4c8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Input (random tensor simulating the embeddings)\n",
    "input_tensor = torch.randn(batch_size, seq_len, d_model)  # Shape: (batch_size, seq_len, d_model)\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7de9f0f-28f2-4b25-a8be-0e39609752c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 10, 512]),\n",
       " torch.Size([32, 10, 512]),\n",
       " torch.Size([32, 10, 512]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Linear Projections for Q, K, and V\n",
    "# Create 3 separate weight matrices for Query, Key, and Value, which will be learned.\n",
    "# Each of these weight matrices will have shape (d_model, d_model) for Q, K, V.\n",
    "\n",
    "W_q = nn.Linear(d_model, d_model)\n",
    "W_k = nn.Linear(d_model, d_model)\n",
    "W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = W_q(input_tensor)  # Shape: (batch_size, seq_len, d_model)\n",
    "K = W_k(input_tensor)  # Shape: (batch_size, seq_len, d_model)\n",
    "V = W_v(input_tensor)  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f7422dd-64a5-43b6-82f9-2f6d06254408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 10, 64]),\n",
       " torch.Size([32, 8, 10, 64]),\n",
       " torch.Size([32, 8, 10, 64]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Splitting into Heads (num_heads = 8, d_head = 64)\n",
    "# Reshape Q, K, and V to split them into heads, which will have shape (batch_size, num_heads, seq_len, d_head).\n",
    "\n",
    "Q = Q.reshape(batch_size, seq_len, num_heads, d_head).permute(0, 2, 1, 3)  # Shape: (batch_size, num_heads, seq_len, d_head)\n",
    "K = K.reshape(batch_size, seq_len, num_heads, d_head).permute(0, 2, 1, 3)  # Shape: (batch_size, num_heads, seq_len, d_head)\n",
    "V = V.reshape(batch_size, seq_len, num_heads, d_head).permute(0, 2, 1, 3)  # Shape: (batch_size, num_heads, seq_len, d_head)\n",
    "\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2cffb48-0a80-4d52-88f1-40c1528465a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 10, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Scaled Dot-Product Attention\n",
    "# Compute attention between Q and K and apply it to V.\n",
    "\n",
    "# Q and K have shape: (batch_size, num_heads, seq_len, d_head)\n",
    "# To compute attention, we take the dot product of Q and K transposed, then scale it by sqrt(d_head)\n",
    "attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / d_head**0.5  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "# Apply softmax to get the attention weights (sum to 1 along the last dimension)\n",
    "attn_weights = F.softmax(attn_scores, dim=-1)  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "# Apply the attention weights to the Value matrix (V)\n",
    "attn_output = torch.matmul(attn_weights, V)  # Shape: (batch_size, num_heads, seq_len, d_head)\n",
    "\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a568b910-dc99-4162-b91d-1beeb5eef864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Concatenate Heads\n",
    "# Concatenate the attention outputs from all heads along the last dimension.\n",
    "attn_output = attn_output.permute(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b5d0d6e-fb93-48ed-b527-fe58a425d932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Final Linear Projection\n",
    "# After concatenation, project the output back to the d_model dimension using a final linear layer\n",
    "\n",
    "W_o = nn.Linear(d_model, d_model)\n",
    "output = W_o(attn_output)  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f3ab968-0ad3-474b-80dc-7bc59ee65209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 10, 512])\n",
      "Q shape: torch.Size([32, 8, 10, 64])\n",
      "K shape: torch.Size([32, 8, 10, 64])\n",
      "V shape: torch.Size([32, 8, 10, 64])\n",
      "Attention scores shape: torch.Size([32, 8, 10, 10])\n",
      "Attention weights shape: torch.Size([32, 8, 10, 10])\n",
      "Attention output shape (after concatenation): torch.Size([32, 10, 512])\n",
      "Final output shape: torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "print(f\"Attention scores shape: {attn_scores.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"Attention output shape (after concatenation): {attn_output.shape}\")\n",
    "print(f\"Final output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c525abc0-f0f9-4ea4-bd28-1e26d508c2d9",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eae170d4-9571-45ab-a9b3-3211242592ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Final output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, embed_dim]\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.q_linear(x)  # [batch_size, seq_len, embed_dim]\n",
    "        k = self.k_linear(x)  # [batch_size, seq_len, embed_dim]\n",
    "        v = self.v_linear(x)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # Split embed_dim into num_heads Ã— head_dim\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # q, k, v shapes: [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1))  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        \n",
    "        # Scale attention scores\n",
    "        scores = scores / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        values = torch.matmul(attention_weights, v)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # Transpose and reshape back\n",
    "        values = values.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        # context shape: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.out_proj(values)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9f766e1-97a3-4fa9-8eab-506c97269167",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "embed_dim = 512\n",
    "    \n",
    "model = MultiHeadAttention(embed_dim=512, num_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bfb8e3b-e078-4153-bedc-65a7137484ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65892ed1-dad5-4bf9-ae95-444ad3443012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Each head dimension: 64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Each head dimension: {embed_dim // 8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02f443-6706-45b2-9078-11b92a49872b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
