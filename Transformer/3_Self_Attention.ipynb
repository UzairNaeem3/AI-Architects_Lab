{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7bcaae00-1e98-4838-bfdb-6203741afbba",
      "metadata": {
        "id": "7bcaae00-1e98-4838-bfdb-6203741afbba"
      },
      "source": [
        "## Self Attention in Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd150c7-56e7-46cd-8b1c-9ced2e10e4b5",
      "metadata": {
        "id": "7bd150c7-56e7-46cd-8b1c-9ced2e10e4b5"
      },
      "source": [
        "Self-attention helps a model **pay attention** to different parts of the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69fe2f43-2e84-4455-9b5e-2f2ce5c78fad",
      "metadata": {
        "id": "69fe2f43-2e84-4455-9b5e-2f2ce5c78fad"
      },
      "source": [
        "### Imagine a Sentence:\n",
        "\n",
        "Take the sentence: **The cat sat on the mat.**\n",
        "\n",
        "When the model reads this sentence, **self-attention** helps it determine which other words are important for understanding each word.\n",
        "\n",
        "For example, when processing the word **\"cat\"**, the model might focus on:\n",
        "- **\"the\"** — to recognize that it's a specific cat.\n",
        "- **\"sat\"** — to understand the action the cat is performing.\n",
        "\n",
        "This allows the model to build a deeper understanding of the sentence by considering how words relate to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78eea498-b141-4ea3-b967-5153c4aa1787",
      "metadata": {
        "id": "78eea498-b141-4ea3-b967-5153c4aa1787"
      },
      "source": [
        "### How Transformers Understand Language: The Complete Attention Mechanism\n",
        "\n",
        "Let's understand how a Transformer processes the sentence: **\"The cat sat on the mat\"**\n",
        "\n",
        "\n",
        "### Step 1: Input Tokenization and Embedding\n",
        "\n",
        "**Process:** The sentence is first broken down into individual tokens (words or sub-words), and each token is converted into a high-dimensional vector representation called an embedding.\n",
        "\n",
        "```\n",
        "Input sentence: \"The cat sat on the mat\"\n",
        "Tokens: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "Token positions: [0, 1, 2, 3, 4, 5]\n",
        "```\n",
        "\n",
        "**Embedding Details:** Each token gets converted into a dense vector, typically of size 512 or 768 dimensions. These embeddings capture semantic meaning learned during training.\n",
        "\n",
        "```\n",
        "\"cat\" → [0.2, -0.1, 0.8, 0.3, ..., 0.5]  (d_model = 512 dimensions)\n",
        "\"sat\" → [0.1, 0.4, -0.2, 0.7, ..., -0.3] (d_model = 512 dimensions)\n",
        "```\n",
        "\n",
        "**Mathematical Representation:**\n",
        "```\n",
        "X has shape (n, d_model)\n",
        "where:\n",
        "  n = sequence length (e.g., 6 tokens in above sentence)\n",
        "  d_model = embedding dimension (e.g., 512)\n",
        "```\n",
        "\n",
        "### Step 2: Creating Query, Key, and Value Vectors\n",
        "\n",
        "The attention mechanism works by transforming each input embedding into three different vector types. This is the core innovation that allows the model to determine what information is relevant.\n",
        "\n",
        "#### Query (Q) Vectors - \"What am I looking for?\"\n",
        "\n",
        "**Explanation:** The Query vector represents what information a particular token is seeking from other tokens in the sequence. Think of it as a search query that each word uses to find relevant context from other words.\n",
        "\n",
        "**Detailed Example:** When processing the word \"cat\", its Query vector encodes questions like:\n",
        "- \"What actions are associated with me?\"\n",
        "- \"What descriptive words modify me?\"\n",
        "- \"What spatial relationships involve me?\"\n",
        "\n",
        "**Mathematical Transformation:**\n",
        "```\n",
        "Q = X × W_q\n",
        "Where:\n",
        " W_q has shape (d_model, d_k)\n",
        " Typically: d_model = 512, d_k = 64  # dimension reduction for efficiency\n",
        " If X has shape (n, d_model), e.g., n = 6\n",
        "\n",
        "Then Q will have shape (n, d_k) = (6, 64)\n",
        "```\n",
        "\n",
        "#### Key (K) Vectors - \"What information do I offer?\"\n",
        "\n",
        "**Explanation:** The Key vector represents what information each token can provide to others. It acts like a searchable index or label that describes the token's content and role in the sentence.\n",
        "\n",
        "**Detailed Example:** The word \"sat\" has a Key vector that announces:\n",
        "- \"I am a past-tense action verb\"\n",
        "- \"I describe what the subject (cat) did\"\n",
        "- \"I connect the subject to a location\"\n",
        "\n",
        "**Mathematical Transformation:**\n",
        "```\n",
        "K = X × W_k\n",
        "Where:\n",
        " W_k has shape (d_model, d_k)\n",
        " Typically: d_model = 512, d_k = 64\n",
        " If X has shape (n, d_model), e.g., n = 6\n",
        "\n",
        "Then K will have shape (n, d_k) = (6, 64)\n",
        "\n",
        "```\n",
        "\n",
        "#### Value (V) Vectors - \"What is my actual content?\"\n",
        "\n",
        "**Explanation:** The Value vector contains the actual information content that will be used in the final representation. If a token is deemed relevant (through Query-Key matching), its Value vector contributes to the output.\n",
        "\n",
        "**Detailed Example:** The Value vector for \"cat\" contains:\n",
        "- Semantic information about animals\n",
        "- Syntactic information about being a noun\n",
        "- Contextual information about its role as a subject\n",
        "\n",
        "**Mathematical Transformation:**\n",
        "```\n",
        "V = X × W_v\n",
        "Where:\n",
        " W_v has shape (d_model, d_v)\n",
        " Typically: d_model = 512, d_k = 64\n",
        " If X has shape (n, d_model), e.g., n = 6\n",
        "\n",
        "Then V will have shape (n, d_v) = (6, 64)\n",
        "```\n",
        "\n",
        "#### Representation of Q, K, V Creation:\n",
        "\n",
        "| Token | Original Embedding | Query Vector | Key Vector | Value Vector |\n",
        "|-------|-------------------|--------------|------------|-------------|\n",
        "| \"The\" | [512 dims] → | [64 dims] | [64 dims] | [64 dims] |\n",
        "| \"cat\" | [512 dims] → | [64 dims] | [64 dims] | [64 dims] |\n",
        "| \"sat\" | [512 dims] → | [64 dims] | [64 dims] | [64 dims] |\n",
        "| \"on\" | [512 dims] → | [64 dims] | [64 dims] | [64 dims] |\n",
        "| \"the\" | [512 dims] → | [64 dims] | [64 dims] | [64 dims] |\n",
        "| \"mat\" | [512 dims] → | [64 dims] | [64 dims] | [64 dims] |\n",
        "\n",
        "\n",
        "### Step 3: Computing Attention Scores\n",
        "\n",
        "**Overview:** For each token, we calculate how much attention it should pay to every other token (including itself) by comparing its Query vector with all Key vectors.\n",
        "\n",
        "#### Similarity Calculation (Dot Product)\n",
        "\n",
        "**Explanation:** The dot product between Query and Key vectors measures their similarity. A high dot product means the Query is looking for exactly what the Key offers, indicating strong relevance.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "```\n",
        "Attention_Scores = Q × K^T\n",
        "Where:\n",
        " Q has shape (n, d_k)\n",
        " K^T (transpose of K) has shape (d_k, n)\n",
        "\n",
        "Resulting Attention_Scores will have shape (n, n) = (6, 6)\n",
        "\n",
        "```\n",
        "\n",
        "**Example - Attention Score Matrix:**\n",
        "```\n",
        "        The   cat   sat   on    the   mat\n",
        "The   [ 2.1   1.2   0.8   0.9   2.0   0.7 ]\n",
        "cat   [ 1.1   3.2   4.1   0.6   1.0   1.8 ]\n",
        "sat   [ 0.9   4.0   2.8   2.1   0.8   2.2 ]\n",
        "on    [ 0.7   1.5   2.3   1.9   1.1   3.1 ]\n",
        "the   [ 1.8   1.0   0.9   1.2   2.1   0.8 ]\n",
        "mat   [ 0.6   2.1   1.9   2.8   0.7   2.5 ]\n",
        "```\n",
        "\n",
        "#### Scaling for Stability\n",
        "\n",
        "**Why Scaling is Needed:** Large dot products can cause the softmax function to have very small gradients, making training difficult. Scaling prevents this issue.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "```\n",
        "Scaled_Scores = Attention_Scores / sqrt(d_k)\n",
        "For d_k = 64: Scaled_Scores = Attention_Scores / sqrt(64) = Attention_Scores / 8\n",
        "```\n",
        "\n",
        "\n",
        "### Step 4: Converting Scores to Probabilities (Softmax)\n",
        "\n",
        "**Purpose:** The softmax function converts raw attention scores into a probability distribution, ensuring all attention weights for a token sum to 1.0.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "```\n",
        "Attention_Weights[i, j] = exp(Scaled_Scores[i, j]) / sum(exp(Scaled_Scores[i, k]) for k in range(n))\n",
        "\n",
        "```\n",
        "\n",
        "**Example - After Softmax:**\n",
        "```\n",
        "Attention weights for \"cat\" looking at all tokens:\n",
        "\"The\" → 0.05  (very low attention)\n",
        "\"cat\" → 0.15  (moderate self-attention)\n",
        "\"sat\" → 0.65  (high attention - verb relates strongly to subject)\n",
        "\"on\"  → 0.03  (very low attention)\n",
        "\"the\" → 0.07  (low attention)\n",
        "\"mat\" → 0.05  (very low attention)\n",
        "\n",
        "Sum = 1.00 (probabilities must sum to 1)\n",
        "```\n",
        "\n",
        "### Step 5: Computing Final Output\n",
        "\n",
        "**Process:** The attention weights are used to compute a weighted average of all Value vectors, creating a context-aware representation for each token.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "```\n",
        "Output = Attention_Weights × V\n",
        "Where:\n",
        " Attention_Weights has shape (n, n)\n",
        " V has shape (n, d_v)\n",
        "\n",
        "Resulting Output will have shape (n, d_v) = (6, 64)\n",
        "\n",
        "```\n",
        "\n",
        "**Detailed Example for \"cat\":**\n",
        "```\n",
        "cat_output = (\n",
        "    0.05 * V_\"The\" +    # Contribution from \"The\"\n",
        "    0.15 * V_\"cat\" +    # Contribution from \"cat\" itself\n",
        "    0.65 * V_\"sat\" +    # Highest attention weight, meaning \"sat\" is most relevant\n",
        "    0.03 * V_\"on\" +     # Smaller contributions from other words\n",
        "    0.07 * V_\"the\" +\n",
        "    0.05 * V_\"mat\"\n",
        ")\n",
        "\n",
        "cat_output is the output vector for the word \"cat\" after applying attention.\n",
        "It is a weighted sum of the value vectors (V) of all words in the sequence.\n",
        "The weights (like 0.05, 0.15, etc.) come from the attention scores for \"cat\".\n",
        "```\n",
        "\n",
        "**Result Interpretation:** The output representation for \"cat\" now includes:\n",
        "- Primarily information from \"sat\" (65% weight) - understanding the action\n",
        "- Some self-information (15% weight) - retaining its identity\n",
        "- Small contributions from other tokens - maintaining broader context\n",
        "\n",
        "\n",
        "### Complete Mathematical Summary\n",
        "\n",
        "**The entire attention mechanism in one formula:**\n",
        "```\n",
        "Attention(Q, K, V) = softmax((Q × K^T) / sqrt(d_k)) × V\n",
        "\n",
        "X shape: (n, d_model) = (6, 512)\n",
        "W_q, W_k, W_v shapes: (d_model, d_k) = (512, 64)\n",
        "\n",
        "Q = X × W_q    # (6, 512) × (512, 64) = (6, 64)\n",
        "K = X × W_k    # (6, 512) × (512, 64) = (6, 64)\n",
        "V = X × W_v    # (6, 512) × (512, 64) = (6, 64)\n",
        "```\n",
        "\n",
        "\n",
        "### Why This Mechanism is Powerful\n",
        "\n",
        "**Parallel Processing:** Unlike RNNs that process tokens sequentially, attention processes all tokens simultaneously, making it much faster and allowing for better parallelization.\n",
        "\n",
        "**Long-Range Dependencies:** Any token can directly attend to any other token, regardless of distance, solving the vanishing gradient problem that plagued earlier architectures.\n",
        "\n",
        "**Dynamic Context:** The attention weights are computed dynamically based on the content, allowing the model to focus on different aspects depending on what's most relevant for the current context.\n",
        "\n",
        "**Multi-Head Attention:** In practice, transformers use multiple attention heads (typically 8-12), each learning to focus on different types of relationships:\n",
        "- Syntactic relationships (subject-verb, adjective-noun)\n",
        "- Semantic relationships (word meanings and associations)\n",
        "- Positional relationships (word order and sequence)\n",
        "- Discourse relationships (coreference)\n",
        "\n",
        "This comprehensive attention mechanism enables Transformers to build rich, contextual representations that capture both local and global dependencies in language."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb5b27a3-eeaf-441b-96ae-ff7bee175b78",
      "metadata": {
        "id": "bb5b27a3-eeaf-441b-96ae-ff7bee175b78"
      },
      "source": [
        "\\begin{equation}\n",
        "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ea4732d-fc18-4d98-8f33-9b28a8f2af53",
      "metadata": {
        "id": "8ea4732d-fc18-4d98-8f33-9b28a8f2af53"
      },
      "source": [
        "#### Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18fc8334-836d-4144-8633-6cdf08a1372f",
      "metadata": {
        "id": "18fc8334-836d-4144-8633-6cdf08a1372f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a053a21-ebb7-422b-9db2-169df6e80483",
      "metadata": {
        "id": "5a053a21-ebb7-422b-9db2-169df6e80483"
      },
      "outputs": [],
      "source": [
        "seq_len, d_k, d_v = 4, 8, 8\n",
        "Q = np.random.randn(seq_len, d_k) # Q (Query): shape (seq_len, d_k) -> (4, 8)\n",
        "K = np.random.randn(seq_len, d_k) # K (Key): shape (seq_len, d_k) -> (4, 8)\n",
        "V = np.random.randn(seq_len, d_v) # V (Value): shape (seq_len, d_v) -> (4, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd5e9ce-4f54-4f36-a212-6f344541c344",
      "metadata": {
        "id": "8fd5e9ce-4f54-4f36-a212-6f344541c344"
      },
      "source": [
        "Here, **seq_len = 4** indicates that the sentence or sequence consists of 4 tokens (words) and **d_k = 8** is the embedding dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf4efe2d-cf88-40d7-9269-e030c915d7a6",
      "metadata": {
        "id": "cf4efe2d-cf88-40d7-9269-e030c915d7a6",
        "outputId": "79ffce83-8866-4284-ffac-d1f65810f3a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " [[ 0.09734245 -1.23944871 -1.95007434 -1.21171088 -1.39929826 -0.31226623\n",
            "   0.31715713 -0.04633441]\n",
            " [-1.08375397  0.66662904 -0.98343286  0.0560969   0.89519004  0.10004913\n",
            "  -1.0552281   0.69236504]\n",
            " [ 0.28716318 -1.82738228  0.81813136 -1.68986197  0.04376673  0.3502005\n",
            "  -0.0423009  -0.41868561]\n",
            " [ 1.50431526 -0.22491201 -0.05686595  0.33269655  0.02673335 -0.1195548\n",
            "  -0.45053287  0.6156462 ]]\n",
            "K\n",
            " [[ 0.86628805  0.94090168 -0.26610346 -0.64998308  1.0927421   0.84253231\n",
            "   0.87369975  1.05338847]\n",
            " [-1.424108   -0.32713153  0.71388877  0.61280272  0.17939416 -0.87583657\n",
            "  -0.52110976 -1.161473  ]\n",
            " [-0.76329217  1.07186251 -0.48726729  0.67996207  0.26643412 -0.89958272\n",
            "   1.88396001  0.49325744]\n",
            " [-1.71032029 -1.1453007  -0.91363038 -0.85842559 -1.24469381 -0.30221369\n",
            "   1.13283269  0.7531194 ]]\n",
            "V\n",
            " [[ 1.07902494 -1.26877275  0.23930751 -0.35502214 -0.95741432  0.12459074\n",
            "   0.76431504  1.23321664]\n",
            " [ 1.7843734  -2.08439949  0.436698    0.21342351  2.24085292 -0.47206793\n",
            "   0.78814499  1.30168928]\n",
            " [-0.93519858  1.11816422 -0.87673241  1.29703975 -0.54234303 -1.90039197\n",
            "   0.46299355 -0.18703436]\n",
            " [ 1.46971655  0.92700179 -1.98057442 -0.17175945  0.12187321  0.31262937\n",
            "   0.12118181  1.22497164]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Q\\n\", Q)\n",
        "print(\"K\\n\", K)\n",
        "print(\"V\\n\", V)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8dba55a-c391-4b21-9249-5f6deb038e27",
      "metadata": {
        "id": "a8dba55a-c391-4b21-9249-5f6deb038e27"
      },
      "source": [
        "#### Self Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20346724-4c88-497a-b77f-37cd5ca19fb2",
      "metadata": {
        "id": "20346724-4c88-497a-b77f-37cd5ca19fb2"
      },
      "source": [
        "\\begin{equation}\n",
        "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf30d97-4c17-476e-8103-4556d4a335f6",
      "metadata": {
        "id": "fdf30d97-4c17-476e-8103-4556d4a335f6",
        "outputId": "dbd1448b-3ec9-4ab8-8595-96d1f7925e50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-1.33923421, -1.95682855, -0.79378427,  6.23532513],\n",
              "       [ 0.78350543,  0.47631351,  0.56110739,  0.12197776],\n",
              "       [-0.72506282, -0.05318572, -4.31516902,  1.78136061],\n",
              "       [ 1.07380337, -2.27622329, -1.56581858, -2.59277681]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.matmul(Q, K.T) # Q has shape (4, 8) and K.T has shape (8, 4), so the result will have shape (4, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96c896c3-133a-404b-aa61-8866dfeedfc9",
      "metadata": {
        "id": "96c896c3-133a-404b-aa61-8866dfeedfc9"
      },
      "source": [
        "**Why we need sqrt(d_k) in denominator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ec5bcd-61e4-4335-a4db-1797e50d12b3",
      "metadata": {
        "id": "28ec5bcd-61e4-4335-a4db-1797e50d12b3",
        "outputId": "6aa00b95-d0db-4d86-e852-630ae4be0fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q variance: 0.7053850744157935\n",
            "K variance: 0.867173339673601\n",
            "Q.K(T) variance: 5.176241421279855\n"
          ]
        }
      ],
      "source": [
        "print(\"Q variance:\", Q.var())\n",
        "print(\"K variance:\", K.var())\n",
        "print(\"Q.K(T) variance:\", np.matmul(Q, K.T).var())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c0af85-2bb0-4147-9d05-47a613945c7c",
      "metadata": {
        "id": "08c0af85-2bb0-4147-9d05-47a613945c7c",
        "outputId": "25dea25c-8d39-4b6c-c280-35c9366bfce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q variance: 0.7053850744157935\n",
            "K variance: 0.867173339673601\n",
            "normalized Q.K(T) variance: 0.6470301776599817\n"
          ]
        }
      ],
      "source": [
        "scaled = np.matmul(Q, K.T) / math.sqrt(d_k)\n",
        "print(\"Q variance:\", Q.var())\n",
        "print(\"K variance:\", K.var())\n",
        "print(\"normalized Q.K(T) variance:\", normalized.var())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfbeb713-cfaf-4325-9a26-ba5721ffd116",
      "metadata": {
        "id": "dfbeb713-cfaf-4325-9a26-ba5721ffd116"
      },
      "source": [
        "**Notice the reduction in variance of the product**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f27ec487-a314-4b92-b62b-5d09db52fbef",
      "metadata": {
        "id": "f27ec487-a314-4b92-b62b-5d09db52fbef",
        "outputId": "b76129cc-5cca-44d5-aa79-62bc226a23d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.47349079, -0.69184337, -0.28064512,  2.20452034],\n",
              "       [ 0.277011  ,  0.16840226,  0.19838142,  0.04312565],\n",
              "       [-0.25634842, -0.01880399, -1.52564264,  0.62980608],\n",
              "       [ 0.37964682, -0.80476646, -0.55360047, -0.91668503]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18abb7d2-9665-4dbb-a82a-6e7aa6e81dd7",
      "metadata": {
        "id": "18abb7d2-9665-4dbb-a82a-6e7aa6e81dd7"
      },
      "source": [
        "#### Masking\n",
        "- This is to ensure words don't get context from words generated in the future.\n",
        "- Not required in the encoders, but required int the decoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a97700e-fa1d-4988-bebc-f7513f9a098d",
      "metadata": {
        "id": "9a97700e-fa1d-4988-bebc-f7513f9a098d",
        "outputId": "e7485125-536b-441b-d894-93004522a531"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 1., 1., 1.],\n",
              "       [0., 0., 1., 1.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
        "mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c285a58-f9a1-4ee5-88c4-c4f30c53aa17",
      "metadata": {
        "id": "9c285a58-f9a1-4ee5-88c4-c4f30c53aa17"
      },
      "source": [
        "The masking dimension is usually the same as the sequence length dimension **seq_len**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7661c28a-7171-4cb6-aea1-b72a8b4316a2",
      "metadata": {
        "id": "7661c28a-7171-4cb6-aea1-b72a8b4316a2",
        "outputId": "1479694e-beea-41cd-8459-e7057fd84535"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask[mask == 1] = -np.inf\n",
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4257f1d-a956-4359-978a-77cd8bd7acf3",
      "metadata": {
        "id": "e4257f1d-a956-4359-978a-77cd8bd7acf3",
        "outputId": "0a2158d1-c584-4316-8866-fd0407efff9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.47349079,        -inf,        -inf,        -inf],\n",
              "       [ 0.277011  ,  0.16840226,        -inf,        -inf],\n",
              "       [-0.25634842, -0.01880399, -1.52564264,        -inf],\n",
              "       [ 0.37964682, -0.80476646, -0.55360047, -0.91668503]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled + mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b7c7ebd-58cd-4751-91fd-7ef0145b4f42",
      "metadata": {
        "id": "7b7c7ebd-58cd-4751-91fd-7ef0145b4f42"
      },
      "source": [
        "#### Softmax\n",
        "\n",
        "$$\n",
        "\\text{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee225fa-a054-4d97-8f42-320c9c727281",
      "metadata": {
        "id": "dee225fa-a054-4d97-8f42-320c9c727281"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x)\n",
        "    sum = exp_x.sum()\n",
        "    softmax = exp_x/sum\n",
        "    return softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42512d3-1d89-4c4e-9a27-f683c01826a8",
      "metadata": {
        "id": "d42512d3-1d89-4c4e-9a27-f683c01826a8"
      },
      "outputs": [],
      "source": [
        "attention = softmax(scaled + mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ded83a33-9546-406a-b414-603d75b11183",
      "metadata": {
        "id": "ded83a33-9546-406a-b414-603d75b11183",
        "outputId": "97a4c629-9c0b-4849-c9ba-91f27a165eea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.07803034, 0.        , 0.        , 0.        ],\n",
              "       [0.16527315, 0.14826346, 0.        , 0.        ],\n",
              "       [0.09695434, 0.12295084, 0.02724707, 0.        ],\n",
              "       [0.18313716, 0.05602635, 0.07202319, 0.05009411]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "153379f0-693c-4876-9f25-d855de03b88c",
      "metadata": {
        "id": "153379f0-693c-4876-9f25-d855de03b88c",
        "outputId": "b6cdf043-47b4-4b86-89b7-592b0188ab60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.08419669, -0.09900277,  0.01867325, -0.0277025 , -0.07470737,\n",
              "         0.00972186,  0.05963976,  0.09622832],\n",
              "       [ 0.44289122, -0.51873434,  0.10429746, -0.02703272,  0.17400172,\n",
              "        -0.04939892,  0.24317385,  0.39681055],\n",
              "       [ 0.29852493, -0.34882499,  0.0530059 ,  0.02716019,  0.16791201,\n",
              "        -0.09774164,  0.18362196,  0.27451335],\n",
              "       [ 0.30384966, -0.22216965, -0.09406749,  0.03175239, -0.08274747,\n",
              "        -0.12484244,  0.22354814,  0.34666975]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_V = np.matmul(attention, V)\n",
        "new_V"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "445975d4-23bb-487b-914f-64152051a022",
      "metadata": {
        "id": "445975d4-23bb-487b-914f-64152051a022"
      },
      "source": [
        "### Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6873c35f-f375-40be-ba4a-62e01f312a35",
      "metadata": {
        "id": "6873c35f-f375-40be-ba4a-62e01f312a35"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
        "\n",
        "def self_attention(Q, K, V, mask=None):\n",
        "  d_k = Q.shape[-1]\n",
        "  normalized = np.matmul(Q, K.T) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    normalized = normalized + mask\n",
        "  attention = softmax(normalized)\n",
        "  out = np.matmul(attention, V)\n",
        "  return out, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93af3580-f14f-4c9e-b77a-e1a8ddd29c7e",
      "metadata": {
        "id": "93af3580-f14f-4c9e-b77a-e1a8ddd29c7e",
        "outputId": "8ae2db66-168f-409a-e708-6c8767a5a75d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " [[-1.55805137e+00  1.74599065e+00  6.13182772e-03  9.16655390e-02\n",
            "  -4.19406664e-01  1.20209171e+00 -3.22952013e-01 -1.57484697e+00]\n",
            " [-6.03090864e-01 -1.39311745e+00  5.41038476e-04  8.68501500e-01\n",
            "   1.01740729e+00 -1.23332745e+00  1.31092668e+00  1.07903594e+00]\n",
            " [ 4.15527794e-01  2.58613492e-01  1.41064692e+00  1.89567492e+00\n",
            "  -1.88668455e-01  1.22294262e+00  1.73747406e+00  1.29566400e+00]\n",
            " [-1.65589298e+00  9.20265360e-01  1.67641560e-01  1.55976837e+00\n",
            "   6.85358948e-02  1.00433167e+00  3.21650406e-01 -1.95328813e-01]]\n",
            "K\n",
            " [[ 0.31691824 -0.74019652 -0.16705587  1.15992309 -1.55371717 -0.86020114\n",
            "   0.75528109 -0.38178461]\n",
            " [ 0.2937555  -1.16828174  0.65689693  0.63906619  0.52966841  0.56031207\n",
            "  -0.1390223  -0.35731796]\n",
            " [-0.14005376  1.02710414  0.37303933 -0.28437786 -1.08703429  0.01543534\n",
            "  -0.38521217  0.77901054]\n",
            " [ 1.85600985  1.40490388 -0.41918586  0.43513747  1.02341881  0.765293\n",
            "   1.34713076  0.31635243]]\n",
            "V\n",
            " [[ 0.99755393  1.56166281  0.2986221   0.47543551  0.81800117  0.05950314\n",
            "  -0.11690738 -0.1013089 ]\n",
            " [-2.39818972  0.24701376 -1.32360787 -0.35113535 -0.05594357  0.58126118\n",
            "  -1.77497531 -0.77299625]\n",
            " [ 1.09046411 -0.02865198 -1.0261337  -0.31014785 -1.80174376 -1.98862992\n",
            "   0.40271034  1.33953195]\n",
            " [-0.2376382  -1.61614974 -1.45608631  0.05206926  0.69175483  0.99239482\n",
            "   1.44958549  0.17417298]]\n",
            "New V\n",
            " [[ 0.99755393  1.56166281  0.2986221   0.47543551  0.81800117  0.05950314\n",
            "  -0.11690738 -0.1013089 ]\n",
            " [-0.51501485  0.97607764 -0.42396908  0.1072554   0.42871928  0.29191024\n",
            "  -0.85546189 -0.40049913]\n",
            " [-0.22436118  0.68157875 -0.62778355 -0.02427045 -0.15812759 -0.26177998\n",
            "  -0.59484692  0.01300509]\n",
            " [-0.12616861  0.02597872 -0.89898794 -0.05266735 -0.16166783 -0.15613996\n",
            "  -0.0130185   0.19736386]]\n",
            "Attention\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.55456921 0.44543079 0.         0.        ]\n",
            " [0.38303625 0.36668508 0.25027867 0.        ]\n",
            " [0.23025927 0.25128354 0.27856753 0.23988965]]\n"
          ]
        }
      ],
      "source": [
        "values, attention = self_attention(Q, K, V, mask=mask)\n",
        "print(\"Q\\n\", Q)\n",
        "print(\"K\\n\", K)\n",
        "print(\"V\\n\", V)\n",
        "print(\"New V\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2ddfb4-92b9-43db-ae07-f5dcde447adc",
      "metadata": {
        "id": "1d2ddfb4-92b9-43db-ae07-f5dcde447adc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}