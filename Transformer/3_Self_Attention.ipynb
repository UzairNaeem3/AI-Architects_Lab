{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcaae00-1e98-4838-bfdb-6203741afbba",
   "metadata": {},
   "source": [
    "## Self Attention in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd150c7-56e7-46cd-8b1c-9ced2e10e4b5",
   "metadata": {},
   "source": [
    "Self-attention helps a model **pay attention** to different parts of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe2f43-2e84-4455-9b5e-2f2ce5c78fad",
   "metadata": {},
   "source": [
    "### Imagine a Sentence:\n",
    "\n",
    "Letâ€™s say we have a sentence: **The cat sat on the mat.**\n",
    "\n",
    "Self-attention helps the model figure out which words in this sentence should be focused on when processing each word. For example, when processing the word **cat**, the model might pay attention to words like **the** (before it) and **sat** (after it) to understand its meaning better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eea498-b141-4ea3-b967-5153c4aa1787",
   "metadata": {},
   "source": [
    "### The Process:\n",
    "\n",
    "1. **Input:** The sentence is broken down into words: \"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\".\n",
    "2. **Embedding:** Each word gets converted into a vector.\n",
    "3. **Queries, Keys, and Values:**\n",
    "   - Each word gets three vectors: Query (Q), Key (K), and Value (V). These are created by multiplying the original word vectors by different weights.\n",
    "   - The **Query (Q)**: What the word is looking for\n",
    "   - The **Key (K)**: What the word offers to others\n",
    "   - The **Value (V)**: The actual information the word contains\n",
    "4. **Attention Mechanism:**\n",
    "   - For each word, it compares its **Query (Q)** vector with the **Key (K)** vectors of all the other words to figure out how much attention each word should get.\n",
    "   - This comparison usually involves calculating a **score** that tells how much attention each word should give to others.\n",
    "   - These scores are then used to calculate a weighted average of the **Value (V)** vectors, which gives the output for that word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b27a3-eeaf-441b-96ae-ff7bee175b78",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4732d-fc18-4d98-8f33-9b28a8f2af53",
   "metadata": {},
   "source": [
    "#### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fc8334-836d-4144-8633-6cdf08a1372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a053a21-ebb7-422b-9db2-169df6e80483",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, d_k, d_v = 4, 8, 8\n",
    "Q = np.random.randn(seq_len, d_k) # Q (Query): shape (seq_len, d_k) -> (4, 8)\n",
    "K = np.random.randn(seq_len, d_k) # K (Key): shape (seq_len, d_k) -> (4, 8)\n",
    "V = np.random.randn(seq_len, d_v) # V (Value): shape (seq_len, d_v) -> (4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd5e9ce-4f54-4f36-a212-6f344541c344",
   "metadata": {},
   "source": [
    "Here, **seq_len = 4** indicates that the sentence or sequence consists of 4 tokens (words) and **d_k = 8** is the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4efe2d-cf88-40d7-9269-e030c915d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.09734245 -1.23944871 -1.95007434 -1.21171088 -1.39929826 -0.31226623\n",
      "   0.31715713 -0.04633441]\n",
      " [-1.08375397  0.66662904 -0.98343286  0.0560969   0.89519004  0.10004913\n",
      "  -1.0552281   0.69236504]\n",
      " [ 0.28716318 -1.82738228  0.81813136 -1.68986197  0.04376673  0.3502005\n",
      "  -0.0423009  -0.41868561]\n",
      " [ 1.50431526 -0.22491201 -0.05686595  0.33269655  0.02673335 -0.1195548\n",
      "  -0.45053287  0.6156462 ]]\n",
      "K\n",
      " [[ 0.86628805  0.94090168 -0.26610346 -0.64998308  1.0927421   0.84253231\n",
      "   0.87369975  1.05338847]\n",
      " [-1.424108   -0.32713153  0.71388877  0.61280272  0.17939416 -0.87583657\n",
      "  -0.52110976 -1.161473  ]\n",
      " [-0.76329217  1.07186251 -0.48726729  0.67996207  0.26643412 -0.89958272\n",
      "   1.88396001  0.49325744]\n",
      " [-1.71032029 -1.1453007  -0.91363038 -0.85842559 -1.24469381 -0.30221369\n",
      "   1.13283269  0.7531194 ]]\n",
      "V\n",
      " [[ 1.07902494 -1.26877275  0.23930751 -0.35502214 -0.95741432  0.12459074\n",
      "   0.76431504  1.23321664]\n",
      " [ 1.7843734  -2.08439949  0.436698    0.21342351  2.24085292 -0.47206793\n",
      "   0.78814499  1.30168928]\n",
      " [-0.93519858  1.11816422 -0.87673241  1.29703975 -0.54234303 -1.90039197\n",
      "   0.46299355 -0.18703436]\n",
      " [ 1.46971655  0.92700179 -1.98057442 -0.17175945  0.12187321  0.31262937\n",
      "   0.12118181  1.22497164]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\", Q)\n",
    "print(\"K\\n\", K)\n",
    "print(\"V\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dba55a-c391-4b21-9249-5f6deb038e27",
   "metadata": {},
   "source": [
    "#### Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20346724-4c88-497a-b77f-37cd5ca19fb2",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf30d97-4c17-476e-8103-4556d4a335f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.33923421, -1.95682855, -0.79378427,  6.23532513],\n",
       "       [ 0.78350543,  0.47631351,  0.56110739,  0.12197776],\n",
       "       [-0.72506282, -0.05318572, -4.31516902,  1.78136061],\n",
       "       [ 1.07380337, -2.27622329, -1.56581858, -2.59277681]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(Q, K.T) # Q has shape (4, 8) and K.T has shape (8, 4), so the result will have shape (4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c896c3-133a-404b-aa61-8866dfeedfc9",
   "metadata": {},
   "source": [
    "**Why we need sqrt(d_k) in denominator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ec5bcd-61e4-4335-a4db-1797e50d12b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q variance: 0.7053850744157935\n",
      "K variance: 0.867173339673601\n",
      "Q.K(T) variance: 5.176241421279855\n"
     ]
    }
   ],
   "source": [
    "print(\"Q variance:\", Q.var()) \n",
    "print(\"K variance:\", K.var())\n",
    "print(\"Q.K(T) variance:\", np.matmul(Q, K.T).var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c0af85-2bb0-4147-9d05-47a613945c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q variance: 0.7053850744157935\n",
      "K variance: 0.867173339673601\n",
      "normalized Q.K(T) variance: 0.6470301776599817\n"
     ]
    }
   ],
   "source": [
    "scaled = np.matmul(Q, K.T) / math.sqrt(d_k)\n",
    "print(\"Q variance:\", Q.var()) \n",
    "print(\"K variance:\", K.var())\n",
    "print(\"normalized Q.K(T) variance:\", normalized.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbeb713-cfaf-4325-9a26-ba5721ffd116",
   "metadata": {},
   "source": [
    "**Notice the reduction in variance of the product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27ec487-a314-4b92-b62b-5d09db52fbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47349079, -0.69184337, -0.28064512,  2.20452034],\n",
       "       [ 0.277011  ,  0.16840226,  0.19838142,  0.04312565],\n",
       "       [-0.25634842, -0.01880399, -1.52564264,  0.62980608],\n",
       "       [ 0.37964682, -0.80476646, -0.55360047, -0.91668503]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abb7d2-9665-4dbb-a82a-6e7aa6e81dd7",
   "metadata": {},
   "source": [
    "#### Masking\n",
    "- This is to ensure words don't get context from words generated in the future.\n",
    "- Not required in the encoders, but required int the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a97700e-fa1d-4988-bebc-f7513f9a098d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1.],\n",
       "       [0., 0., 1., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c285a58-f9a1-4ee5-88c4-c4f30c53aa17",
   "metadata": {},
   "source": [
    "The masking dimension is usually the same as the sequence length dimension **seq_len**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7661c28a-7171-4cb6-aea1-b72a8b4316a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 1] = -np.inf\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4257f1d-a956-4359-978a-77cd8bd7acf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47349079,        -inf,        -inf,        -inf],\n",
       "       [ 0.277011  ,  0.16840226,        -inf,        -inf],\n",
       "       [-0.25634842, -0.01880399, -1.52564264,        -inf],\n",
       "       [ 0.37964682, -0.80476646, -0.55360047, -0.91668503]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c7ebd-58cd-4751-91fd-7ef0145b4f42",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dee225fa-a054-4d97-8f42-320c9c727281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    sum = exp_x.sum()\n",
    "    softmax = exp_x/sum\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d42512d3-1d89-4c4e-9a27-f683c01826a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ded83a33-9546-406a-b414-603d75b11183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07803034, 0.        , 0.        , 0.        ],\n",
       "       [0.16527315, 0.14826346, 0.        , 0.        ],\n",
       "       [0.09695434, 0.12295084, 0.02724707, 0.        ],\n",
       "       [0.18313716, 0.05602635, 0.07202319, 0.05009411]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "153379f0-693c-4876-9f25-d855de03b88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08419669, -0.09900277,  0.01867325, -0.0277025 , -0.07470737,\n",
       "         0.00972186,  0.05963976,  0.09622832],\n",
       "       [ 0.44289122, -0.51873434,  0.10429746, -0.02703272,  0.17400172,\n",
       "        -0.04939892,  0.24317385,  0.39681055],\n",
       "       [ 0.29852493, -0.34882499,  0.0530059 ,  0.02716019,  0.16791201,\n",
       "        -0.09774164,  0.18362196,  0.27451335],\n",
       "       [ 0.30384966, -0.22216965, -0.09406749,  0.03175239, -0.08274747,\n",
       "        -0.12484244,  0.22354814,  0.34666975]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_V = np.matmul(attention, V)\n",
    "new_V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445975d4-23bb-487b-914f-64152051a022",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6873c35f-f375-40be-ba4a-62e01f312a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "def self_attention(Q, K, V, mask=None):\n",
    "  d_k = Q.shape[-1]\n",
    "  normalized = np.matmul(Q, K.T) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    normalized = normalized + mask\n",
    "  attention = softmax(normalized)\n",
    "  out = np.matmul(attention, V)\n",
    "  return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93af3580-f14f-4c9e-b77a-e1a8ddd29c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-1.55805137e+00  1.74599065e+00  6.13182772e-03  9.16655390e-02\n",
      "  -4.19406664e-01  1.20209171e+00 -3.22952013e-01 -1.57484697e+00]\n",
      " [-6.03090864e-01 -1.39311745e+00  5.41038476e-04  8.68501500e-01\n",
      "   1.01740729e+00 -1.23332745e+00  1.31092668e+00  1.07903594e+00]\n",
      " [ 4.15527794e-01  2.58613492e-01  1.41064692e+00  1.89567492e+00\n",
      "  -1.88668455e-01  1.22294262e+00  1.73747406e+00  1.29566400e+00]\n",
      " [-1.65589298e+00  9.20265360e-01  1.67641560e-01  1.55976837e+00\n",
      "   6.85358948e-02  1.00433167e+00  3.21650406e-01 -1.95328813e-01]]\n",
      "K\n",
      " [[ 0.31691824 -0.74019652 -0.16705587  1.15992309 -1.55371717 -0.86020114\n",
      "   0.75528109 -0.38178461]\n",
      " [ 0.2937555  -1.16828174  0.65689693  0.63906619  0.52966841  0.56031207\n",
      "  -0.1390223  -0.35731796]\n",
      " [-0.14005376  1.02710414  0.37303933 -0.28437786 -1.08703429  0.01543534\n",
      "  -0.38521217  0.77901054]\n",
      " [ 1.85600985  1.40490388 -0.41918586  0.43513747  1.02341881  0.765293\n",
      "   1.34713076  0.31635243]]\n",
      "V\n",
      " [[ 0.99755393  1.56166281  0.2986221   0.47543551  0.81800117  0.05950314\n",
      "  -0.11690738 -0.1013089 ]\n",
      " [-2.39818972  0.24701376 -1.32360787 -0.35113535 -0.05594357  0.58126118\n",
      "  -1.77497531 -0.77299625]\n",
      " [ 1.09046411 -0.02865198 -1.0261337  -0.31014785 -1.80174376 -1.98862992\n",
      "   0.40271034  1.33953195]\n",
      " [-0.2376382  -1.61614974 -1.45608631  0.05206926  0.69175483  0.99239482\n",
      "   1.44958549  0.17417298]]\n",
      "New V\n",
      " [[ 0.99755393  1.56166281  0.2986221   0.47543551  0.81800117  0.05950314\n",
      "  -0.11690738 -0.1013089 ]\n",
      " [-0.51501485  0.97607764 -0.42396908  0.1072554   0.42871928  0.29191024\n",
      "  -0.85546189 -0.40049913]\n",
      " [-0.22436118  0.68157875 -0.62778355 -0.02427045 -0.15812759 -0.26177998\n",
      "  -0.59484692  0.01300509]\n",
      " [-0.12616861  0.02597872 -0.89898794 -0.05266735 -0.16166783 -0.15613996\n",
      "  -0.0130185   0.19736386]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.55456921 0.44543079 0.         0.        ]\n",
      " [0.38303625 0.36668508 0.25027867 0.        ]\n",
      " [0.23025927 0.25128354 0.27856753 0.23988965]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = self_attention(Q, K, V, mask=mask)\n",
    "print(\"Q\\n\", Q)\n",
    "print(\"K\\n\", K)\n",
    "print(\"V\\n\", V)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ddfb4-92b9-43db-ae07-f5dcde447adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
