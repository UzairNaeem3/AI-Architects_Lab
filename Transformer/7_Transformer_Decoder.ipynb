{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7936b0e-281e-4e26-bd42-0a531682cc41",
   "metadata": {},
   "source": [
    "### Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f713180-fabf-4497-a263-3db308899337",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://res.cloudinary.com/edlitera/image/upload/c_fill,f_auto/v1680629118/blog/gz5ccspg3yvq4eo6xhrr\" alt=\"Transformer Decoder\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8128f21-e708-4935-b8c2-252d4b115b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9329f-b3fc-4242-8557-c42a92f16844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing classes from the respective notebooks\n",
    "%run 4_Multihead_Attention.ipynb\n",
    "%run 5_FeedForward.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016b7a5-efb0-4a75-9274-4a6424061be5",
   "metadata": {},
   "source": [
    "**Attention**:\n",
    "- `Self-Attention Layer`: Allows each position in the decoder to attend to all previous positions (using causal masking)\n",
    "- `Cross-Attention Layer`: Allows the decoder to attend to the encoder's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "429e3fab-921a-4c28-b924-2e65b260243f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Single transformer decoder layer with masked self-attention, \n",
    "        cross-attention, and feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Dimension of embeddings\n",
    "            num_heads: Number of attention heads\n",
    "            ff_hidden_dim: Hidden dimension of feed-forward network\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.ff = FeedForward(embed_dim, ff_hidden_dim, dropout)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of decoder layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, embed_dim]\n",
    "            enc_output: Encoder output [batch_size, enc_seq_len, embed_dim]\n",
    "            self_attn_mask: Mask for self-attention (usually causal mask)\n",
    "            cross_attn_mask: Mask for cross-attention\n",
    "            \n",
    "        Returns:\n",
    "            x: Output tensor [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        # Self-attention block with residual connection and layer normalization\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = residual + self.dropout(self.self_attn(x, x, x, self_attn_mask))\n",
    "        \n",
    "        # Cross-attention block with residual connection and layer normalization\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = residual + self.dropout(self.cross_attn(x, enc_output, enc_output, cross_attn_mask))\n",
    "        \n",
    "        # Feed-forward block with residual connection and layer normalization\n",
    "        residual = x\n",
    "        x = self.norm3(x)\n",
    "        x = residual + self.dropout(self.ff(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd9fa86e-b892-429a-89eb-d081207aa150",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "embed_dim = 64\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 256\n",
    "dropout = 0.1\n",
    "\n",
    "decoder_layer = DecoderLayer(embed_dim, num_heads, ff_hidden_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "379cd96c-85b0-43b1-8b51-9a068b31d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input tensors for the forward pass\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "enc_seq_len = 12\n",
    "\n",
    "# Random input tensors for the example\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)  # Decoder input (e.g., previous tokens embeddings)\n",
    "enc_output = torch.randn(batch_size, enc_seq_len, embed_dim)  # Encoder output (e.g., encoder's final hidden states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19aaf137-9ba1-4715-b86d-f0765a524ba4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a simple causal mask for self-attention\n",
    "self_attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()  # Upper triangular matrix (future tokens should be masked)\n",
    "\n",
    "self_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28d75669-cd1d-4b0d-87b0-430d622e08d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the mask compatible with the batch size by unsqueezing it\n",
    "self_attn_mask = self_attn_mask.unsqueeze(0).expand(batch_size, -1, -1)  # Shape [batch_size, seq_len, seq_len]\n",
    "self_attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8bfd786-ccda-436a-803c-06d46cd890e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_attn_mask = None  # Could be used to mask out certain encoder tokens, for now it's None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a912e93a-a34b-4094-87bf-bb8748cc9cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass through the decoder layer\n",
    "output = decoder_layer(x, enc_output, self_attn_mask=self_attn_mask, cross_attn_mask=cross_attn_mask)\n",
    "\n",
    "print(output.shape)  # Should be [batch_size, seq_len, embed_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13151142-363d-4a8f-98df-d2da568b0385",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Attention:\n",
    "- **Self-Attention (in the Decoder)**: This is where each token in the target sequence (tgt) can look at earlier tokens in the sequence (but not future tokens). This is done using a causal mask. The model can only look at **previous tokens** in the target sequence, and the **causal mask** prevents it from looking at future tokens.\n",
    "- **Cross-Attention (in the Decoder)**: This is where the target sequence (tgt) attends to the source sequence (src). The decoder can attend to any token in the source, but it should not attend to padding tokens in the source. No causal mask is needed here because we are not concerned with future tokens in the source. The model attends to the **source sequence** but **ignores padding tokens** in the source. There is no causal mask in cross-attention because we donâ€™t need to worry about future tokens in the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f256ba-394b-4ef8-a437-66903fac29cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
